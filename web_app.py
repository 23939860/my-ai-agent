import streamlit as st
from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.tools.tavily_search import TavilySearchResults
from datetime import datetime
from langchain.agents import create_react_agent, AgentExecutor, Tool
from langchain import hub
import os
import tempfile

# ======================
# å®‰å…¨è·å– API Key
# ======================
if "DASHSCOPE_API_KEY" not in st.secrets:
    st.error("âŒ è¯·åœ¨ Streamlit Cloud çš„ Secrets ä¸­è®¾ç½® DASHSCOPE_API_KEY")
    st.stop()

QWEN_API_KEY = st.secrets["DASHSCOPE_API_KEY"]

# åˆå§‹åŒ– LLMï¼ˆç”¨äºèŠå¤©å’Œ PDF é—®ç­”ï¼‰
llm = ChatOpenAI(
    model="qwen-max",
    api_key=QWEN_API_KEY,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# ======================
# Streamlit é¡µé¢è®¾ç½®
# ======================
st.set_page_config(page_title="ğŸ¤– AI æ™ºèƒ½åŠ©æ‰‹ï¼ˆReAct + PDFï¼‰", layout="wide")
st.title("ğŸ¤– AI æ™ºèƒ½åŠ©æ‰‹ï¼ˆReAct Agent + Qwen + PDFï¼‰")

# åˆå§‹åŒ– session_state
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "messages" not in st.session_state:
    st.session_state.messages = []
if "agent_executor" not in st.session_state:
    # è‡ªå®šä¹‰è·å–å½“å‰æ—¶é—´çš„å·¥å…·
    def get_current_time(*args, **kwargs):
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    tools = [
        TavilySearchResults(
            max_results=3,
            api_key=st.secrets["TAVILY_API_KEY"]  # æ˜¾å¼ä¼ å…¥å¯†é’¥
        ),
        Tool(
            name="CurrentTime",
            func=get_current_time,
            description="è·å–å½“å‰æ—¥æœŸå’Œæ—¶é—´"
        )
    ]
    react_prompt = hub.pull("hwchase17/react")
    agent = create_react_agent(llm, tools, react_prompt)
    st.session_state.agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# ======================
# PDF ä¸Šä¼ åŒº
# ======================
with st.expander("ğŸ“ ä¸Šä¼  PDF æ–‡æ¡£ï¼ˆå¯é€‰ï¼‰"):
    uploaded_file = st.file_uploader("é€‰æ‹©ä¸€ä¸ª PDF æ–‡ä»¶", type=["pdf"], label_visibility="collapsed")

    if uploaded_file and st.session_state.vectorstore is None:
        with st.spinner("æ­£åœ¨è§£æ PDF..."):
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_path = tmp_file.name
            
            try:
                loader = UnstructuredPDFLoader(tmp_path)
                docs = loader.load()
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
                splits = text_splitter.split_documents(docs)
                
                from langchain_community.embeddings import HuggingFaceEmbeddings
                embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/all-MiniLM-L6-v2"
                )
                vectorstore = FAISS.from_documents(splits, embeddings)
                st.session_state.vectorstore = vectorstore
                st.success("âœ… PDF è§£æå®Œæˆï¼ç°åœ¨å¯ä»¥æé—®æ–‡æ¡£å†…å®¹ã€‚")
            except Exception as e:
                st.error(f"âŒ PDF è§£æå¤±è´¥: {e}")
            finally:
                os.unlink(tmp_path)

# ======================
# èŠå¤©åŒº
# ======================
for msg in st.session_state.messages:
    if msg["role"] == "user":
        st.chat_message("user").write(msg["content"])
    else:
        st.chat_message("assistant").write(msg["content"])

if prompt := st.chat_input("ä¾‹å¦‚ï¼š'ç°åœ¨å‡ ç‚¹ï¼Ÿ' æˆ– 'è¿™ä»½æ–‡æ¡£è®²äº†ä»€ä¹ˆï¼Ÿ'"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            try:
                if st.session_state.vectorstore is not None:
                    retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": 3})
                    template = """
                    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚
                    å¦‚æœä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·è¯´â€œæ ¹æ®æ–‡æ¡£å†…å®¹æ— æ³•ç¡®å®šâ€ã€‚
                    
                    ä¸Šä¸‹æ–‡ï¼š
                    {context}
                    
                    é—®é¢˜ï¼š{question}
                    """
                    prompt_template = ChatPromptTemplate.from_template(template)
                    rag_chain = (
                        {"context": retriever, "question": RunnablePassthrough()}
                        | prompt_template
                        | llm
                        | StrOutputParser()
                    )
                    response = rag_chain.invoke(prompt)
                else:
                    response = st.session_state.agent_executor.invoke({"input": prompt})["output"]
                
                st.write(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
                
            except Exception as e:
                error_msg = f"æŠ±æ­‰ï¼Œå¤„ç†æ—¶å‡ºé”™ï¼š{str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})